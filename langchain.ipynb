{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daed49ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\moiza\\AppData\\Local\\anaconda3\\envs\\volmatica\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"<think>\\nOkay, the user is asking for the capital of Pakistan. Let me start by recalling what I know. Pakistan's capital is Islamabad. I should confirm that to make sure. Sometimes people might confuse it with Karachi, which is a major city and the largest one, but Karachi used to be the capital before Islamabad was built. When did that happen? I think Islamabad became the capital in 1960. The reason for moving was to have a more centrally located capital, which makes sense for administrative purposes.\\n\\nI need to check if there's any other city that could be mistaken as the capital. Lahore is another big city, but it's in Punjab and not the capital. So the key points here are Islamabad as the current capital, established in 1960, replacing Karachi. Also, maybe mention that it's a planned city, designed specifically for the role. That adds context about why it was chosen. The user might appreciate knowing a bit more than just the name. Let me make sure there are no other capitals in different provinces. For example, Punjab's capital is Lahore, Sindh's is Karachi, Khyber Pakhtunkhwa's is Peshawar, and Balochistan's is Quetta. But the federal capital is Islamabad. \\n\\nWait, I should also mention that Islamabad is in the Islamabad Capital Territory, which is separate from the provinces. That clarifies its administrative status. Also, maybe include that it's near Rawalpindi, which is another significant city nearby. People might confuse Rawalpindi with the capital too. So the answer should clearly state Islamabad as the capital, the year it became the capital, maybe a bit about its planned nature, and perhaps differentiate it from other major cities. Let me structure that into a concise response without being too verbose. Make sure the main point is clear upfront and then add supporting details. Double-check the facts to avoid errors. Yeah, Islamabad is definitely the correct answer here.\\n</think>\\n\\nThe capital of Pakistan is **Islamabad**. It was established as the capital in 1960, replacing Karachi, and is a planned city designed to serve as the federal administrative center. Located in the **Islamabad Capital Territory**, it is distinct from the four provinces and is situated near the city of Rawalpindi. \\n\\n**Key details:**\\n- **Established:** 1960.\\n- **Purpose:** Chosen for its central location and security, away from coastal vulnerabilities.\\n- **Notable Features:** Houses key government institutions, including the Parliament and the Faisal Mosque.\\n- **Other Major Cities:** Karachi (former capital and largest city), Lahore (cultural hub), and Peshawar/Quetta (provincial capitals). \\n\\nLet me know if you'd like further details! ðŸŒðŸ‡µðŸ‡°\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 577, 'prompt_tokens': 12, 'total_tokens': 589, 'completion_time': 1.5477316719999998, 'completion_tokens_details': None, 'prompt_time': 0.000278336, 'prompt_tokens_details': None, 'queue_time': 0.006277134, 'total_time': 1.5480100079999999}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--019bc1d8-4cbe-76e2-871b-c869c93e3089-0', usage_metadata={'input_tokens': 12, 'output_tokens': 577, 'total_tokens': 589})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "model = init_chat_model(\"groq:qwen/qwen3-32b\")\n",
    "response = model.invoke(\"Capital of pak?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ac293e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbc255f",
   "metadata": {},
   "source": [
    "### Creating ChatModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "937889a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\moiza\\AppData\\Local\\anaconda3\\envs\\volmatica\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model_name = \"llama-3.3-70b-versatile\",\n",
    "    temperature = 0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a089aa57",
   "metadata": {},
   "source": [
    "### Without prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e4f715f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langchain is a technology framework that helps build applications powered by large language models (like chatbots or virtual assistants). \n",
      "\n",
      "Imagine you have a super smart friend who can answer any question and do many tasks for you. But, this friend can only understand and respond to text-based inputs. Langchain is like a system that helps you talk to this smart friend more efficiently and get the most out of their abilities.\n",
      "\n",
      "It does a few key things:\n",
      "\n",
      "1. **Asks the right questions**: Langchain helps figure out what you want to ask or do, and then asks the smart friend (the language model) the right questions to get the best answer.\n",
      "2. **Understands the context**: It keeps track of the conversation, so the smart friend knows what you're talking about and can give more accurate responses.\n",
      "3. **Combines answers**: If you need information from multiple sources or tasks, Langchain can combine the answers from the smart friend to give you a complete response.\n",
      "4. **Makes it easier to build apps**: Langchain provides a set of tools and templates that make it easier for developers to build applications that use large language models, like chatbots or virtual assistants.\n",
      "\n",
      "In simple words, Langchain is like a bridge that connects you to a super smart language model, helping you have a more productive and efficient conversation, and making it easier for developers to build cool apps that use this technology.\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(\"Explain Langchain in simple words\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075ef161",
   "metadata": {},
   "source": [
    "### Creating PromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3783f685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='LangChain is a framework that helps build applications using large language models (like AI chatbots). Here\\'s a simplified explanation:\\n\\n**What is LangChain?**\\nLangChain is like a set of building blocks that allows developers to create more powerful and interactive AI applications. It\\'s designed to work with large language models, which are AI systems that can understand and generate human-like text.\\n\\n**Key features:**\\n\\n1. **Modular architecture**: LangChain breaks down the development process into smaller, manageable pieces. This makes it easier for developers to build, test, and customize their AI applications.\\n2. **Language model integration**: LangChain provides a simple way to connect large language models to other tools and services, allowing developers to tap into their capabilities.\\n3. **Chain-based workflow**: LangChain uses a \"chain\" concept, where multiple language models and tools are linked together to perform complex tasks. This enables developers to create more sophisticated AI workflows.\\n\\n**Benefits:**\\n\\n1. **Faster development**: LangChain\\'s modular design and pre-built components speed up the development process, allowing developers to focus on building innovative applications.\\n2. **Improved flexibility**: LangChain\\'s framework makes it easier to experiment with different language models, tools, and workflows, giving developers more flexibility and creative freedom.\\n3. **Enhanced capabilities**: By integrating multiple language models and tools, LangChain enables developers to build more powerful and interactive AI applications that can perform a wide range of tasks.\\n\\n**In simple terms**: LangChain is a tool that helps developers build more advanced AI applications by providing a flexible framework, pre-built components, and easy integration with large language models. This allows developers to focus on creating innovative and interactive AI experiences.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 344, 'prompt_tokens': 47, 'total_tokens': 391, 'completion_time': 1.199535511, 'completion_tokens_details': None, 'prompt_time': 0.001432459, 'prompt_tokens_details': None, 'queue_time': 0.008684665, 'total_time': 1.20096797}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_c06d5113ec', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None} id='run--019bc1f6-a3fa-7553-b2cf-4c17add908ca-0' usage_metadata={'input_tokens': 47, 'output_tokens': 344, 'total_tokens': 391}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "\n",
    "    (\"system\", \"You are a helpful assistant\"),\n",
    "    (\"user\", \"Explain {topic} in simple words\") #topic is variable (reusable)\n",
    "]) #still not receive any response\n",
    "\n",
    "#giving value to prompt\n",
    "messages = prompt.format_messages(topic = \"LangChain\")\n",
    "\n",
    "#Model\n",
    "llm = ChatGroq(\n",
    "    model_name = \"llama-3.3-70b-versatile\",\n",
    "    temperature = 0.7\n",
    ")\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24889207",
   "metadata": {},
   "source": [
    "### Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6fb790",
   "metadata": {},
   "source": [
    "1. Simplest\n",
    "    - Single Prompt Template + Single LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ddc7adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moiza\\AppData\\Local\\Temp\\ipykernel_8780\\3133183335.py:15: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(llm = llm, prompt = prompt)\n",
      "C:\\Users\\moiza\\AppData\\Local\\Temp\\ipykernel_8780\\3133183335.py:18: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = chain.run(topic = \"langchain.ipynb\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`langchain.ipynb` is a Jupyter Notebook file that demonstrates the use of the LangChain library. LangChain is a Python library that helps you build and interact with large language models (LLMs) like chatbots or virtual assistants.\n",
      "\n",
      "In simple words, `langchain.ipynb` is a tutorial or a guide that shows you how to use LangChain to:\n",
      "\n",
      "1. **Load a language model**: Connect to a pre-trained language model, like a chatbot or a conversational AI.\n",
      "2. **Interact with the model**: Send messages or prompts to the model and get responses back.\n",
      "3. **Chain multiple models**: Combine multiple language models to create a more complex conversation flow.\n",
      "4. **Use tools and agents**: Utilize built-in tools and agents to perform tasks, such as text processing, sentiment analysis, or even generating code.\n",
      "\n",
      "The notebook (`langchain.ipynb`) provides a step-by-step guide on how to use LangChain, including:\n",
      "\n",
      "* Importing the necessary libraries\n",
      "* Loading a language model\n",
      "* Defining a conversation flow\n",
      "* Sending and receiving messages\n",
      "* Using tools and agents to enhance the conversation\n",
      "\n",
      "By following the notebook, you can learn how to build your own conversational AI applications using LangChain and large language models.\n",
      "\n",
      "Is there a specific part of `langchain.ipynb` you'd like me to explain further?\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "#1. PromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful teacher\"),\n",
    "    (\"user\", \"Explain {topic} in simple words\")\n",
    "])\n",
    "\n",
    "#2. Model\n",
    "llm = ChatGroq(model_name = \"llama-3.3-70b-versatile\", temperature = 0.7)\n",
    "\n",
    "#3. LLM Chain\n",
    "chain = LLMChain(llm = llm, prompt = prompt)\n",
    "\n",
    "#4. Run Chain with input variable\n",
    "response = chain.run(topic = \"langchain.ipynb\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1e8909",
   "metadata": {},
   "source": [
    "### Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "05e101dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'resume_text': 'Moiz Ali has 1 year of experience with python, fastapi, docker', 'skills': 'Here are the skills extracted from the resume:\\n\\n1. Python\\n2. FastAPI\\n3. Docker\\n\\nLet me know if you need any further assistance!', 'experience': 'According to the given information, Moiz Ali has a total experience of 1 year.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.chains import LLMChain, SequentialChain\n",
    "\n",
    "#1. Model\n",
    "llm = ChatGroq(model_name = \"llama-3.3-70b-versatile\", temperature = 0.7)\n",
    "\n",
    "#2. Prompt Template\n",
    "\n",
    "#prompt for skills\n",
    "prompt_skills = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an ATS assistant\"),\n",
    "    (\"user\", \"Extract all skills from this resume: {resume_text}\")\n",
    "])\n",
    "\n",
    "# Prompt for experience\n",
    "prompt_experience = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an ATS assistant\"),\n",
    "    (\"user\", \"Extract total experience in years from this resume: {resume_text}\")\n",
    "])\n",
    "\n",
    "\n",
    "#3. LLM Chain\n",
    "chain_skills = LLMChain(llm = llm, prompt = prompt_skills, output_key = \"skills\")\n",
    "chain_experience = LLMChain(llm = llm, prompt = prompt_experience, output_key = \"experience\")\n",
    "\n",
    "#4. Sequential Chain\n",
    "seq_chain = SequentialChain(\n",
    "    chains = [chain_skills, chain_experience],\n",
    "    input_variables = [\"resume_text\"], #initial input\n",
    "    output_variables = [\"skills\", \"experience\"] #final output\n",
    ")\n",
    "\n",
    "#5. Run\n",
    "resume_text = \"Moiz Ali has 1 year of experience with python, fastapi, docker\"\n",
    "result = seq_chain({\"resume_text\": resume_text})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7446533",
   "metadata": {},
   "source": [
    "### Router Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "43b02bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the skills extracted from the resume:\n",
      "\n",
      "1. Programming languages:\n",
      "\t* Python\n",
      "2. Frameworks:\n",
      "\t* FastAPI\n",
      "3. Containerization:\n",
      "\t* Docker\n",
      "\n",
      "Let me know if you'd like me to help with anything else.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "#1. LLM\n",
    "llm = ChatGroq(model_name=\"llama-3.3-70b-versatile\", temperature=0.7)\n",
    "\n",
    "#2. PromptTemplates\n",
    "prompt_skills = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an ATS assistant.\"),\n",
    "    (\"user\", \"Extract skills from this resume: {resume_text}\")\n",
    "])\n",
    "\n",
    "prompt_experience = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an ATS assistant.\"),\n",
    "    (\"user\", \"Extract total experience from this resume: {resume_text}\")\n",
    "])\n",
    "\n",
    "#3. LLMChains\n",
    "chain_skills = LLMChain(llm=llm, prompt=prompt_skills, output_key=\"skills\")\n",
    "chain_experience = LLMChain(llm=llm, prompt=prompt_experience, output_key=\"experience\")\n",
    "\n",
    "#4. Router function (custom)\n",
    "def router_chain(user_input, resume_text):\n",
    "    if \"skill\" in user_input.lower():\n",
    "        return chain_skills.run(resume_text=resume_text)\n",
    "    elif \"experience\" in user_input.lower():\n",
    "        return chain_experience.run(resume_text=resume_text)\n",
    "    else:\n",
    "        return \"Cannot classify input.\"\n",
    "\n",
    "#5. Run\n",
    "resume_text = \"Moiz Ali has 1 year experience with Python, FastAPI, Docker.\"\n",
    "user_input = \"Please list all skills of Moiz Ali.\"\n",
    "\n",
    "result = router_chain(user_input=user_input, resume_text=resume_text)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e93d4c5",
   "metadata": {},
   "source": [
    "### Output Parser\n",
    "1. Simple Output Parser\n",
    "2. JSON Output Parser\n",
    "3. Custom Python Function Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ca8f7a",
   "metadata": {},
   "source": [
    "#### JSON Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "feb49972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'current': 'Ankara', 'previous': 'Istanbul', 'history': [{'capital': 'Bursa', 'period': '1299â€“1326'}, {'capital': 'Edirne', 'period': '1326â€“1328'}, {'capital': 'Istanbul', 'period': '1328â€“1923'}, {'capital': 'Ankara', 'period': '1923â€“present'}]}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate,ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from typer import prompt\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "Api_Key = os.getenv('GROQ_API_KEY')\n",
    "def chatOpenAICli(Model_name = 'openai/gpt-oss-20b' , base_url = 'https://api.groq.com/openai/v1'\n",
    "                  , temperature = 0.2):\n",
    "    return ChatOpenAI(model_name = Model_name, base_url = base_url, temperature = temperature, api_key = Api_Key)\n",
    "if __name__ == \"__main__\":\n",
    "    parser = JsonOutputParser()\n",
    "    cli = chatOpenAICli()\n",
    "    prompt = ChatPromptTemplate(\n",
    "        input_variables=[\"question\",'DateTime','model'],\n",
    "        messages=[\n",
    "            (\"system\", \"You are a highly intelligent {model} answering bot at {DateTime}.\"),\n",
    "            (\"user\", \"{question}\"),]  )\n",
    "    chain = prompt | cli | parser\n",
    "    response = chain.invoke({'model': 'Teacher', \"DateTime\": '1800', \"question\": \"What is the capital of Turkey? return the 'history' and the current and previous capitals as a json object with keys 'current' and 'previous' capitals amd 'history'.\"})\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b09dc0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'skills': ['Python', 'FastAPI', 'Docker', 'Django']}\n",
      "{'experience': 3}\n",
      "\n",
      "Full ATS Parsed Result: {'skills': ['Python', 'FastAPI', 'Docker', 'Django'], 'experience': 3}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "#1. Load API key\n",
    "load_dotenv()\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "#2. Initialize LLM\n",
    "llm = ChatGroq(\n",
    "    model_name=\"llama-3.3-70b-versatile\",\n",
    "    temperature=0.5,\n",
    "    api_key=GROQ_API_KEY\n",
    ")\n",
    "\n",
    "#3. Prompt Templates\n",
    "\n",
    "# Skills extraction prompt\n",
    "prompt_skills = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an ATS assistant extracting skills from resumes.\"),\n",
    "    (\"user\", \"\"\"Extract all skills from this resume as JSON ONLY.\n",
    "Example: {{\"skills\": [\"Python\", \"FastAPI\", \"Docker\"]}}\n",
    "Resume: {resume_text}\"\"\")\n",
    "])\n",
    "\n",
    "# Experience extraction prompt\n",
    "prompt_experience = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an ATS assistant extracting work experience.\"),\n",
    "    (\"user\", \"\"\"Extract total experience in years from this resume as JSON ONLY.\n",
    "Example: {{\"experience\": 2}}\n",
    "Resume: {resume_text}\"\"\")\n",
    "])\n",
    "\n",
    "#4. JSON Output Parsers\n",
    "skills_parser = JsonOutputParser(schema={\"skills\": \"List of skills extracted from resume\"})\n",
    "experience_parser = JsonOutputParser(schema={\"experience\": \"Total experience in years\"})\n",
    "\n",
    "#5. Example Resume\n",
    "resume_text = \"Moiz Ali has 3 years experience in Python, FastAPI, Docker, Django.\"\n",
    "\n",
    "# Skills\n",
    "msg_skills = prompt_skills.format_messages(resume_text=resume_text)\n",
    "raw_skills = llm.invoke(msg_skills).content\n",
    "parsed_skills = skills_parser.parse(raw_skills)\n",
    "print(parsed_skills)\n",
    "\n",
    "# Experience\n",
    "msg_exp = prompt_experience.format_messages(resume_text=resume_text)\n",
    "raw_exp = llm.invoke(msg_exp).content\n",
    "parsed_exp = experience_parser.parse(raw_exp)\n",
    "print(parsed_exp)\n",
    "\n",
    "#6. Combined Output\n",
    "result = {\n",
    "    \"skills\": parsed_skills[\"skills\"],\n",
    "    \"experience\": parsed_exp[\"experience\"]\n",
    "}\n",
    "\n",
    "print(\"\\nFull ATS Parsed Result:\", result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ca62ef",
   "metadata": {},
   "source": [
    "### Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c352d1cb",
   "metadata": {},
   "source": [
    "#### Conversation Buffer Memory\n",
    "- Conversation Buffer only store messages\n",
    "- LLM's (Groq, or other custom models) we explicitly inject history\n",
    "- If we use ChatOpenAI or GPT-3.5-turbo then it stores automatic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f88defb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(/hello) Hello Moiz, it's nice to meet you. Is there something I can help you with or would you like to chat?\n",
      " I don't have any information about your name. I'm a large language model, I don't have the ability to know your personal details unless you tell me. If you'd like to share your name, I'd be happy to chat with you and get to know you better.\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# LLM\n",
    "llm = ChatGroq(model_name=\"llama-3.3-70b-versatile\", temperature=0.7)\n",
    "\n",
    "# Memory\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# Prompt\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"user\", \"{user_input}\"),\n",
    "    (\"assistant\", \"{chat_history}\")\n",
    "])\n",
    "\n",
    "# Chain\n",
    "chat_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "# Run multi-turn conversation\n",
    "print(chat_chain.run(user_input=\"Hello, my name is moiz\"))\n",
    "print(chat_chain.run(user_input=\"Can you tell me my name?\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9554a4a7",
   "metadata": {},
   "source": [
    "### Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5fb23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia Tool Output:\n",
      " Venezuela, officially the Bolivarian Republic of Venezuela, is a country on the northern coast of South America, consisting of a continental landmass and various islands and islets in the Caribbean Sea. It comprises an area of 912,050 km2 (352,140 sq mi), with a population estimated at 31.8 million in 2025.\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import Tool\n",
    "from langchain_groq import ChatGroq\n",
    "import wikipedia\n",
    "\n",
    "#1. LLM\n",
    "llm = ChatGroq(model_name=\"llama-3.3-70b-versatile\", temperature=0.7)\n",
    "\n",
    "#2. Python function to fetch Wikipedia summary\n",
    "def wiki_summary(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Returns a short summary from Wikipedia for the given query.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return wikipedia.summary(query, sentences=2)\n",
    "    except Exception as e:\n",
    "        return f\"Could not find info: {e}\"\n",
    "\n",
    "#3. Create Tool\n",
    "wiki_tool = Tool(\n",
    "    name=\"Wikipedia\",\n",
    "    func=wiki_summary,\n",
    "    description=\"Fetches a 2-sentence summary of a topic from Wikipedia. Input: topic name\"\n",
    ")\n",
    "\n",
    "#4. Use the tool\n",
    "topic = \"Venezuela\"\n",
    "summary = wiki_tool.run(topic)\n",
    "print(\"Wikipedia Tool Output:\\n\", summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479474a6",
   "metadata": {},
   "source": [
    "#### DuckDuckGoSearchRun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9eca8a31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Venezuela, [e] officially the Bolivarian Republic of Venezuela, [f] is a country on the northern coast of South America, consisting of a continental landmass and various islands and islets in the â€¦ The process of merging most of the unidentified parties involved in the pro- Bolivarian Revolution coalition was initiated by Venezuelan president ... More than 70% of Venezuela 's food is imported; 140 Venezuela became so dependent on food imports that it could no longer afford when the price of oil ... Venezuela fue avistada por primera vez durante el tercer viaje de CristÃ³bal ColÃ³n , el 1 de agosto de 1498, cuando llegÃ³ a la desembocadura del ... 3 days ago Â· Stay on top of Venezuela latest developments on the ground with Al Jazeeraâ€™s fact-based news, exclusive video footage, photos and updated maps.\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "search = DuckDuckGoSearchRun(max_results=1)\n",
    "r = search.invoke(\"Venezuela\")\n",
    "r"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "volmatica",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
