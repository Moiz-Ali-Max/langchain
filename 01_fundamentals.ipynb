{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a00c0a8",
   "metadata": {},
   "source": [
    "### Chat Models\n",
    "A chat model is an LLM that:\n",
    "- Accepts messages\n",
    "- Returns messages\n",
    "\n",
    "#### Why LangChain wraps LLMs?\n",
    "\n",
    "Because in production:\n",
    "- You want retries\n",
    "- You want consistent interface\n",
    "- You want to switch models easily\n",
    "\n",
    "```\n",
    "ChatGroq ‚Üí LangChain wrapper\n",
    "\n",
    "invoke() ‚Üí single LLM call\n",
    "\n",
    ".content ‚Üí clean text output\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e972cc26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**LangChain** is an open‚Äësource framework that makes it easier to build applications powered by large language models (LLMs) such as OpenAI‚Äôs GPT‚Äë4, Anthropic‚Äôs Claude, LLaMA, Gemini, and many others.  \n",
      "\n",
      "---\n",
      "\n",
      "## Core Idea\n",
      "\n",
      "Instead of treating an LLM as a single ‚Äúblack‚Äëbox‚Äù that takes a prompt and returns text, LangChain encourages you to **compose** the LLM with other pieces of logic‚Äîdata sources, memory, agents, tools, and control flow‚Äîso you can create more sophisticated, reliable, and interactive AI systems. Think of it as a ‚Äútoolkit for chaining together‚Äù the building blocks that a real‚Äëworld LLM app needs.\n",
      "\n",
      "---\n",
      "\n",
      "## Main Building Blocks\n",
      "\n",
      "| Block | What it does | Typical use‚Äëcase |\n",
      "|-------|--------------|-----------------|\n",
      "| **LLM Wrapper** | Uniform Python (or JS/TS) interface to any LLM provider (OpenAI, Azure, Cohere, HuggingFace, etc.) | Send prompts, get completions, stream tokens |\n",
      "| **Prompt Templates** | Structured, reusable prompt strings with variable substitution, partial formatting, and output parsers | Standardize prompts for QA, summarization, classification |\n",
      "| **Chains** | Sequential pipelines that pass the output of one component as the input to the next | ‚ÄúRetrieve ‚Üí Summarize ‚Üí Refine‚Äù pipelines |\n",
      "| **Agents** | Decision‚Äëmaking loops that can call tools (search APIs, calculators, DB queries) based on LLM‚Äëgenerated reasoning | Chatbots that can browse the web, run code, or fetch data on the fly |\n",
      "| **Memory** | State‚Äëkeeping across turns (conversation history, short‚Äëterm context, long‚Äëterm vector store) | Conversational assistants that remember prior user inputs |\n",
      "| **Document Loaders** | Connectors for reading data from PDFs, Word docs, CSVs, webpages, S3, databases, etc. | Ingesting knowledge bases for retrieval‚Äëaugmented generation |\n",
      "| **Vector Stores** | Indexes (FAISS, Pinecone, Weaviate, Milvus, Chroma, etc.) that store embeddings for similarity search | Retrieval‚Äëaugmented generation (RAG) |\n",
      "| **Output Parsers** | Structured‚Äëoutput utilities (JSON, Pydantic models, regex) that turn raw LLM text into typed data | Extracting structured answers, function calls |\n",
      "| **Callbacks & Tracing** | Hooks for logging, monitoring, and visualizing the flow of a chain/agent | Debugging, observability, cost tracking |\n",
      "\n",
      "---\n",
      "\n",
      "## Typical Workflow (RAG Example)\n",
      "\n",
      "1. **Load Documents** ‚Üí `DocumentLoader` reads PDFs, webpages, etc.  \n",
      "2. **Chunk & Embed** ‚Üí Split into chunks, embed with an embedding model, store in a **VectorStore**.  \n",
      "3. **Retriever** ‚Üí Given a user query, perform similarity search to fetch relevant chunks.  \n",
      "4. **Prompt Template** ‚Üí Insert retrieved chunks into a prompt that tells the LLM to answer the question using that context.  \n",
      "5. **LLM Call** ‚Üí Generate the final answer.  \n",
      "6. **Post‚ÄëProcessing** ‚Üí Parse the answer, add citations, log usage.\n",
      "\n",
      "All of these steps can be wired together with a single `RetrievalQAChain` object in just a few lines of code.\n",
      "\n",
      "```python\n",
      "from langchain import OpenAI, PromptTemplate, RetrievalQA\n",
      "from langchain.document_loaders import PyPDFLoader\n",
      "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
      "from langchain.embeddings import OpenAIEmbeddings\n",
      "from langchain.vectorstores import FAISS\n",
      "\n",
      "# 1. Load & split\n",
      "loader = PyPDFLoader(\"my_report.pdf\")\n",
      "docs = loader.load_and_split(text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000))\n",
      "\n",
      "# 2. Embed & index\n",
      "embeddings = OpenAIEmbeddings()\n",
      "vectorstore = FAISS.from_documents(docs, embeddings)\n",
      "\n",
      "# 3. Retriever\n",
      "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
      "\n",
      "# 4. Prompt\n",
      "prompt = PromptTemplate(\n",
      "    template=\"Answer the question using only the following context:\\n\\n{context}\\n\\nQuestion: {question}\",\n",
      "    input_variables=[\"context\", \"question\"]\n",
      ")\n",
      "\n",
      "# 5. Chain\n",
      "qa = RetrievalQA.from_chain_type(\n",
      "    llm=OpenAI(),\n",
      "    chain_type=\"stuff\",\n",
      "    retriever=retriever,\n",
      "    return_source_documents=True,\n",
      "    chain_type_kwargs={\"prompt\": prompt}\n",
      ")\n",
      "\n",
      "# 6. Ask\n",
      "result = qa({\"query\": \"What are the key findings of the report?\"})\n",
      "print(result[\"answer\"])\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "## Why Use LangChain?\n",
      "\n",
      "| Benefit | Explanation |\n",
      "|---------|-------------|\n",
      "| **Modularity** | Swap out any component (LLM, vector store, prompt) without rewriting the whole app. |\n",
      "| **Productivity** | Pre‚Äëbuilt chains (e.g., `ConversationalRetrievalChain`, `SQLDatabaseChain`) let you prototype in minutes. |\n",
      "| **Scalability** | Works with cloud vector stores, async LLM calls, and can be deployed on serverless, Kubernetes, or even locally. |\n",
      "| **Observability** | Built‚Äëin callbacks for logging prompts, token usage, latency, and tracing via LangSmith (LangChain‚Äôs SaaS monitoring platform). |\n",
      "| **Community & Ecosystem** | Hundreds of integrations, a vibrant GitHub repo, extensive docs, and a growing set of ‚ÄúLangChain Hub‚Äù reusable components. |\n",
      "\n",
      "---\n",
      "\n",
      "## Common Use‚ÄëCases\n",
      "\n",
      "| Domain | Example Application |\n",
      "|--------|---------------------|\n",
      "| **Customer Support** | Chatbot that retrieves relevant knowledge‚Äëbase articles, remembers prior tickets, and can trigger ticket‚Äëcreation APIs. |\n",
      "| **Enterprise Search** | RAG over internal documents (intranets, PDFs, code repos) to answer employee queries. |\n",
      "| **Data Analysis** | Agent that can run SQL queries, plot charts, and explain results in natural language. |\n",
      "| **Automation** | Agent that can schedule meetings, send emails, or interact with internal tools based on user instructions. |\n",
      "| **Education** | Tutor that can fetch textbook excerpts, generate practice problems, and keep track of a student‚Äôs progress. |\n",
      "| **Creative Writing** | Chain that drafts outlines, expands sections, and rewrites with style constraints. |\n",
      "\n",
      "---\n",
      "\n",
      "## Getting Started\n",
      "\n",
      "1. **Install**  \n",
      "\n",
      "   ```bash\n",
      "   pip install langchain openai   # plus any vector store you need, e.g., faiss-cpu\n",
      "   ```\n",
      "\n",
      "2. **Set API keys** (environment variables or `langchain` config)\n",
      "\n",
      "   ```bash\n",
      "   export OPENAI_API_KEY=sk-...\n",
      "   ```\n",
      "\n",
      "3. **Run a quick demo** (the ‚ÄúHello World‚Äù of LangChain is a simple LLM call with a prompt template)\n",
      "\n",
      "   ```python\n",
      "   from langchain import OpenAI, PromptTemplate, LLMChain\n",
      "\n",
      "   prompt = PromptTemplate(\n",
      "       template=\"You are a helpful assistant. Answer in one sentence: {question}\",\n",
      "       input_variables=[\"question\"]\n",
      "   )\n",
      "   chain = LLMChain(llm=OpenAI(), prompt=prompt)\n",
      "\n",
      "   print(chain.run({\"question\": \"What is the capital of France?\"}))\n",
      "   ```\n",
      "\n",
      "4. **Explore the docs & examples**  \n",
      "\n",
      "   - Official docs: https://python.langchain.com  \n",
      "   - LangChain Hub (shareable components): https://smith.langchain.com/hub  \n",
      "   - Example notebooks: https://github.com/langchain-ai/langchain/tree/master/examples  \n",
      "\n",
      "---\n",
      "\n",
      "## TL;DR\n",
      "\n",
      "- **LangChain = a modular framework for building LLM‚Äëpowered applications.**  \n",
      "- It lets you **chain together** prompts, retrieval, memory, agents, and external tools in a clean, reusable way.  \n",
      "- With a few lines of code you can go from ‚Äúcall an LLM‚Äù to ‚Äúsearch a vector store, remember conversation history, and invoke APIs‚Äù‚Äîall while staying provider‚Äëagnostic and observable.  \n",
      "\n",
      "If you‚Äôre looking to move beyond simple prompt‚Äëcompletion scripts and want a production‚Äëready, extensible stack for AI assistants, chatbots, RAG systems, or autonomous agents, LangChain is currently the most widely adopted toolkit for that purpose.\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"openai/gpt-oss-120b\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "response = llm.invoke(\"What is LangChain?\")\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1b47eb",
   "metadata": {},
   "source": [
    "### Prompts\n",
    "A prompt is instructions for the LLM, not just a question.\n",
    "\n",
    "Prompt = role + task + rules + format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413e3040",
   "metadata": {},
   "source": [
    "#### Simple Prompt\n",
    "Problem \n",
    "\n",
    "- Can‚Äôt reuse\n",
    "- Can‚Äôt change dynamically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60cee460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## What is Python?\n",
      "\n",
      "Python is a **programming language** ‚Äì a set of words and rules that let you tell a computer what to do.  \n",
      "Think of it like a very precise form of English that a computer can understand.\n",
      "\n",
      "- **Easy to read:** Python code looks a lot like plain English, so it‚Äôs great for beginners.  \n",
      "- **Very popular:** People use it for web sites, games, data analysis, artificial‚Äëintelligence, automation, and more.  \n",
      "- **Free and open‚Äësource:** You can download it for free and use it on Windows, macOS, Linux, and even on phones or tiny devices like Raspberry‚ÄØPi.\n",
      "\n",
      "---\n",
      "\n",
      "## How do you write and run Python?\n",
      "\n",
      "1. **Install Python** ‚Äì go to <https://python.org> and download the installer for your operating system.  \n",
      "2. **Write code** ‚Äì you can use a simple text editor (Notepad, TextEdit) or a special program called an *IDE* (e.g., VS‚ÄØCode, PyCharm, Thonny). Save your file with the extension `.py`, for example `hello.py`.  \n",
      "3. **Run it** ‚Äì open a terminal/command prompt, navigate to the folder where the file is, and type  \n",
      "\n",
      "   ```bash\n",
      "   python hello.py\n",
      "   ```\n",
      "\n",
      "   The computer will execute the instructions inside the file and show any output.\n",
      "\n",
      "---\n",
      "\n",
      "## The building blocks\n",
      "\n",
      "### 1. Print ‚Äì showing something on the screen\n",
      "```python\n",
      "print(\"Hello, world!\")\n",
      "```\n",
      "`print()` tells the computer to display the text inside the quotes.\n",
      "\n",
      "### 2. Variables ‚Äì containers for data\n",
      "```python\n",
      "age = 10          # a number\n",
      "name = \"Alice\"    # a piece of text (called a string)\n",
      "```\n",
      "You can think of a variable as a labeled box where you store a value. You can later change what‚Äôs inside the box.\n",
      "\n",
      "### 3. Data types (kinds of values)\n",
      "\n",
      "| Type | Example | What it means |\n",
      "|------|---------|---------------|\n",
      "| `int` | `42` | Whole numbers |\n",
      "| `float` | `3.14` | Numbers with a decimal point |\n",
      "| `str` | `\"hello\"` | Text (strings) |\n",
      "| `bool` | `True` or `False` | Yes/No, true/false |\n",
      "| `list` | `[1, 2, 3]` | An ordered collection of items |\n",
      "| `dict` | `{\"name\":\"Bob\",\"age\":5}` | A collection of *key ‚Üí value* pairs |\n",
      "\n",
      "### 4. Simple math\n",
      "```python\n",
      "a = 5\n",
      "b = 2\n",
      "print(a + b)   # addition ‚Üí 7\n",
      "print(a * b)   # multiplication ‚Üí 10\n",
      "print(a / b)   # division ‚Üí 2.5\n",
      "print(a // b)  # floor division ‚Üí 2 (whole part only)\n",
      "print(a ** b)  # exponent ‚Üí 25 (5 to the power of 2)\n",
      "```\n",
      "\n",
      "### 5. Decision making ‚Äì `if` statements\n",
      "```python\n",
      "temperature = 30\n",
      "if temperature > 25:\n",
      "    print(\"It's hot!\")\n",
      "else:\n",
      "    print(\"It's not that hot.\")\n",
      "```\n",
      "*Indentation* (the spaces at the start of a line) matters in Python ‚Äì it tells the computer which lines belong together.\n",
      "\n",
      "### 6. Loops ‚Äì doing something many times\n",
      "\n",
      "**`for` loop (over a list or range)**\n",
      "```python\n",
      "for i in range(5):          # i will be 0,1,2,3,4\n",
      "    print(\"Count:\", i)\n",
      "```\n",
      "\n",
      "**`while` loop (repeat while a condition is true)**\n",
      "```python\n",
      "count = 0\n",
      "while count < 3:\n",
      "    print(\"Hello\")\n",
      "    count = count + 1      # make sure to change something, otherwise the loop would run forever\n",
      "```\n",
      "\n",
      "### 7. Functions ‚Äì reusable pieces of code\n",
      "```python\n",
      "def greet(name):            # define a function called greet that takes one argument\n",
      "    print(\"Hello, \" + name + \"!\")\n",
      "\n",
      "greet(\"Sam\")                # call the function\n",
      "greet(\"Lily\")\n",
      "```\n",
      "A function lets you write a set of instructions once and use it many times with different inputs.\n",
      "\n",
      "### 8. Importing other code (modules)\n",
      "Python comes with many **standard libraries** that add extra features. For example, to get random numbers:\n",
      "\n",
      "```python\n",
      "import random\n",
      "\n",
      "dice = random.randint(1, 6)   # random integer between 1 and 6\n",
      "print(\"You rolled:\", dice)\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "## A tiny project: Guess the Number\n",
      "\n",
      "```python\n",
      "import random\n",
      "\n",
      "secret = random.randint(1, 20)   # computer picks a number\n",
      "print(\"I'm thinking of a number between 1 and 20.\")\n",
      "\n",
      "while True:\n",
      "    guess = int(input(\"Take a guess: \"))   # ask the user for a number\n",
      "    if guess < secret:\n",
      "        print(\"Too low!\")\n",
      "    elif guess > secret:\n",
      "        print(\"Too high!\")\n",
      "    else:\n",
      "        print(\"You got it! üéâ\")\n",
      "        break          # exit the loop when the guess is correct\n",
      "```\n",
      "\n",
      "What does this program do?\n",
      "\n",
      "1. Picks a secret number.\n",
      "2. Repeatedly asks you to guess.\n",
      "3. Tells you if your guess is too low, too high, or correct.\n",
      "4. Stops when you guess right.\n",
      "\n",
      "Try typing it into a file called `guess.py` and run it with `python guess.py`. You‚Äôll see how the concepts (variables, loops, conditionals, functions from a module) work together.\n",
      "\n",
      "---\n",
      "\n",
      "## Tips for learning Python\n",
      "\n",
      "| Tip | Why it helps |\n",
      "|-----|--------------|\n",
      "| **Write code every day** | Programming is a skill ‚Äì practice makes it stick. |\n",
      "| **Start small** | Solve tiny problems (e.g., ‚Äúprint the first 10 numbers‚Äù) before tackling big projects. |\n",
      "| **Read error messages** | Python tells you what went wrong; reading them teaches you how to fix bugs. |\n",
      "| **Use the interactive REPL** (`python` command with no file) | You can type one line at a time and see the result instantly. |\n",
      "| **Explore the official tutorial** (<https://docs.python.org/3/tutorial/>) | It‚Äôs written for beginners and covers everything step‚Äëby‚Äëstep. |\n",
      "| **Join a community** (Stack Overflow, Reddit r/learnpython, Discord servers) | You‚Äôll get help when you‚Äôre stuck and see how others solve problems. |\n",
      "\n",
      "---\n",
      "\n",
      "## Bottom line\n",
      "\n",
      "- **Python = simple, readable language** that lets you tell a computer what to do.\n",
      "- You write **scripts** (`.py` files) using **variables, data types, control flow, functions, and modules**.\n",
      "- Run them with the `python` command, and the computer follows your instructions.\n",
      "- With a little practice, you can build anything from a calculator to a web app or a game.\n",
      "\n",
      "Happy coding! üöÄ If you have a specific question or want to see more examples, just ask.\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(\n",
    "    \"Explain Python to a beginner in simple English\"\n",
    ")\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9000eb",
   "metadata": {},
   "source": [
    "### PromptTemplate\n",
    "Prompts are reused\n",
    "- Inputs change\n",
    "- Structure stays same\n",
    "\n",
    "```\n",
    "{topic} is a variable\n",
    "\n",
    "Same prompt ‚Üí many inputs\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c95f74b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**What is LangChain?**  \n",
      "Think of LangChain as a **tool‚Äëbox** that helps you build programs that can talk, think, and act using large language models (LLMs) like ChatGPT, Claude, or Llama. Instead of writing all the low‚Äëlevel code yourself, LangChain gives you ready‚Äëmade building blocks that you can snap together‚Äîjust like LEGO bricks.\n",
      "\n",
      "---\n",
      "\n",
      "## 1. Why do we need it?\n",
      "\n",
      "- **LLMs are powerful, but they‚Äôre just one piece of a bigger puzzle.**  \n",
      "  An LLM can generate text, but most real‚Äëworld apps also need to:\n",
      "  - remember past conversations,\n",
      "  - look up information from a database or the web,\n",
      "  - do calculations,\n",
      "  - follow a series of steps (a ‚Äúchain‚Äù) to reach a result.\n",
      "\n",
      "- **LangChain does the plumbing for you.**  \n",
      "  It connects the LLM to other tools (APIs, memory, prompts, etc.) so you can focus on *what* you want the app to do, not *how* to wire everything together.\n",
      "\n",
      "---\n",
      "\n",
      "## 2. Core ideas (in plain English)\n",
      "\n",
      "| Concept | What it means | Simple analogy |\n",
      "|---------|---------------|----------------|\n",
      "| **LLM (Large Language Model)** | The brain that writes or understands text. | A very smart writer. |\n",
      "| **Prompt** | The instruction you give the LLM. | A question or a command you hand to the writer. |\n",
      "| **Chain** | A series of steps that run one after another. | A recipe: first chop onions, then saut√©, then add sauce. |\n",
      "| **Agent** | An LLM that can decide which tool to use next, like a little robot with a brain. | A chef who can look up a recipe, ask a sous‚Äëchef, or fetch ingredients as needed. |\n",
      "| **Memory** | A place to store past information so the LLM can refer back to it. | A notebook where the chef writes down what‚Äôs already cooked. |\n",
      "| **Tool** | Anything external the LLM can call (search engine, calculator, database, etc.). | Kitchen appliances: blender, oven, thermometer. |\n",
      "\n",
      "---\n",
      "\n",
      "## 3. How a simple LangChain app might look\n",
      "\n",
      "### Example: ‚ÄúAsk‚Äëabout‚Äëyour‚Äëcalendar‚Äù bot\n",
      "\n",
      "1. **User asks:** ‚ÄúWhat meetings do I have tomorrow?‚Äù  \n",
      "2. **Prompt** tells the LLM: *‚ÄúYou are a helpful assistant. Use the calendar data to answer.‚Äù*  \n",
      "3. **Tool call** ‚Äì the LLM decides it needs the calendar, so LangChain calls the Google Calendar API.  \n",
      "4. **Memory** ‚Äì the bot stores today‚Äôs date so it knows ‚Äútomorrow‚Äù means the next day.  \n",
      "5. **Chain** ‚Äì the steps (prompt ‚Üí tool ‚Üí format answer) are linked together automatically.  \n",
      "6. **Response** ‚Äì ‚ÄúYou have a 10‚ÄØam team sync and a 2‚ÄØpm client call.‚Äù\n",
      "\n",
      "All of those steps are written in a few lines of Python (or JavaScript) thanks to LangChain‚Äôs pre‚Äëmade components.\n",
      "\n",
      "---\n",
      "\n",
      "## 4. What you actually write (very simplified)\n",
      "\n",
      "```python\n",
      "from langchain import LLMChain, PromptTemplate, OpenAI, Tool\n",
      "\n",
      "# 1Ô∏è‚É£ Define the prompt (the instruction)\n",
      "prompt = PromptTemplate(\n",
      "    input_variables=[\"question\"],\n",
      "    template=\"You are a friendly assistant. Answer the question: {question}\"\n",
      ")\n",
      "\n",
      "# 2Ô∏è‚É£ Choose the language model\n",
      "llm = OpenAI(model=\"gpt-4\")\n",
      "\n",
      "# 3Ô∏è‚É£ Create a chain that ties prompt + model together\n",
      "chain = LLMChain(prompt=prompt, llm=llm)\n",
      "\n",
      "# 4Ô∏è‚É£ Run it\n",
      "answer = chain.run({\"question\": \"What is the capital of France?\"})\n",
      "print(answer)   # ‚Üí \"Paris\"\n",
      "```\n",
      "\n",
      "That‚Äôs it‚ÄîLangChain handles the ‚Äúglue‚Äù between the prompt and the model. When you need more features (memory, tool calls, agents), you just add the corresponding component.\n",
      "\n",
      "---\n",
      "\n",
      "## 5. When you‚Äôd use LangChain\n",
      "\n",
      "- **Chatbots** that need to remember earlier messages.  \n",
      "- **Question‚Äëanswering systems** that pull data from a database or the web.  \n",
      "- **Automation scripts** that combine LLM reasoning with APIs (e.g., ‚Äúsummarize the latest news and email me‚Äù).  \n",
      "- **Research assistants** that can search papers, extract key points, and keep notes.  \n",
      "\n",
      "In short, any app where the LLM is only one part of a larger workflow.\n",
      "\n",
      "---\n",
      "\n",
      "## 6. Quick recap (bullet points)\n",
      "\n",
      "- **LangChain = a framework** that makes it easy to build LLM‚Äëpowered apps.  \n",
      "- It provides **ready‚Äëmade pieces**: prompts, chains, agents, memory, tools.  \n",
      "- You **connect** those pieces like LEGO bricks to create a workflow.  \n",
      "- The result is a **clean, reusable, and extensible** codebase, so you spend time on ideas, not plumbing.\n",
      "\n",
      "---\n",
      "\n",
      "### TL;DR\n",
      "\n",
      "> **LangChain is a library that helps you stitch together a language model, prompts, memory, and external tools into a working application, without having to write all the low‚Äëlevel integration code yourself.**  \n",
      "\n",
      "Think of it as the ‚Äúrecipe book and kitchen tools‚Äù for building AI‚Äëpowered software. Happy building!\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Explain {topic} to a beginner in simple English\"\n",
    ")\n",
    "\n",
    "formatted_prompt = prompt.format(topic=\"LangChain\")\n",
    "response = llm.invoke(formatted_prompt)\n",
    "\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6c03e2",
   "metadata": {},
   "source": [
    "### Chains\n",
    "A Chain connects Prompt + LLM.\n",
    "\n",
    "#### What LangChain did:\n",
    "\n",
    "1. Took input\n",
    "2. Filled prompt\n",
    "3. Called LLM\n",
    "4. Returned output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab34c55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moiza\\AppData\\Local\\Temp\\ipykernel_2180\\365076371.py:3: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(\n",
      "C:\\Users\\moiza\\AppData\\Local\\Temp\\ipykernel_2180\\365076371.py:8: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  output = chain.run(\"LangChain\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**What is LangChain?**  \n",
      "Think of LangChain as a **tool‚Äëbox** that helps you build programs that can talk, think, and act using large language models (LLMs) like ChatGPT, Claude, or Llama. Instead of writing all the low‚Äëlevel code yourself, LangChain gives you ready‚Äëmade building blocks that you can snap together‚Äîjust like LEGO bricks.\n",
      "\n",
      "---\n",
      "\n",
      "## 1. Why do we need it?\n",
      "\n",
      "- **LLMs are powerful, but they‚Äôre just one piece of a bigger puzzle.**  \n",
      "  An LLM can generate text, but most real‚Äëworld apps also need to:\n",
      "  - remember past conversations,\n",
      "  - look up information from a database or the web,\n",
      "  - do calculations,\n",
      "  - follow a series of steps (a ‚Äúchain‚Äù) to reach a result.\n",
      "\n",
      "- **LangChain does the plumbing for you.**  \n",
      "  It connects the LLM to other tools (APIs, memory, prompts, etc.) so you can focus on *what* you want the app to do, not *how* to wire everything together.\n",
      "\n",
      "---\n",
      "\n",
      "## 2. Core ideas (in plain English)\n",
      "\n",
      "| Concept | What it means | Simple analogy |\n",
      "|---------|---------------|----------------|\n",
      "| **LLM (Large Language Model)** | The brain that writes or understands text. | A very smart writer. |\n",
      "| **Prompt** | The instruction you give the LLM. | A question or a command you hand to the writer. |\n",
      "| **Chain** | A series of steps that run one after another. | A recipe: first chop onions, then saut√©, then add sauce. |\n",
      "| **Agent** | An LLM that can decide which tool to use next, like a little robot with a brain. | A chef who can look up a recipe, fetch ingredients, and cook. |\n",
      "| **Memory** | A place to store past information so the LLM can recall it later. | A notebook where the chef writes down what‚Äôs already cooked. |\n",
      "| **Tool** | Anything external the LLM can call (search engine, calculator, database, etc.). | A kitchen appliance‚Äîblender, oven, thermometer. |\n",
      "\n",
      "---\n",
      "\n",
      "## 3. How a simple LangChain app might look\n",
      "\n",
      "### Example: ‚ÄúAsk‚Äëabout‚Äëyour‚Äëcalendar‚Äù bot\n",
      "\n",
      "1. **User asks:** ‚ÄúWhat meetings do I have tomorrow?‚Äù  \n",
      "2. **Prompt** tells the LLM: *‚ÄúYou are a helpful assistant. Use the calendar API to find events for the user‚Äôs date.‚Äù*  \n",
      "3. **Agent** sees that the question needs a calendar lookup, so it calls the **calendar tool** (an API).  \n",
      "4. The tool returns a list of events.  \n",
      "5. The LLM formats the answer: ‚ÄúYou have a 10‚ÄØam team stand‚Äëup and a 2‚ÄØpm client call.‚Äù  \n",
      "6. **Memory** stores the fact that the user asked about tomorrow, so if they later say ‚ÄúMove the 2‚ÄØpm call to 3‚ÄØpm,‚Äù the bot already knows which event to edit.\n",
      "\n",
      "All of those steps are built from LangChain components, and you only write a few lines of code to glue them together.\n",
      "\n",
      "---\n",
      "\n",
      "## 4. What you actually write (very simplified)\n",
      "\n",
      "```python\n",
      "from langchain import LLMChain, PromptTemplate, OpenAI\n",
      "from langchain.agents import initialize_agent, Tool\n",
      "\n",
      "# 1Ô∏è‚É£ Choose a language model\n",
      "llm = OpenAI(api_key=\"YOUR_KEY\")\n",
      "\n",
      "# 2Ô∏è‚É£ Define a prompt template\n",
      "prompt = PromptTemplate(\n",
      "    input_variables=[\"question\"],\n",
      "    template=\"You are a helpful assistant. Answer the question: {question}\"\n",
      ")\n",
      "\n",
      "# 3Ô∏è‚É£ Create a chain (prompt ‚Üí LLM)\n",
      "chain = LLMChain(llm=llm, prompt=prompt)\n",
      "\n",
      "# 4Ô∏è‚É£ Define a tool (e.g., a fake calendar lookup)\n",
      "def get_events(date):\n",
      "    return \"10‚ÄØam stand‚Äëup, 2‚ÄØpm client call\"\n",
      "\n",
      "calendar_tool = Tool(name=\"Calendar\", func=get_events, description=\"Get events for a date\")\n",
      "\n",
      "# 5Ô∏è‚É£ Build an agent that can use the tool\n",
      "agent = initialize_agent([calendar_tool], llm, agent_type=\"zero-shot-react-description\")\n",
      "\n",
      "# 6Ô∏è‚É£ Run it\n",
      "answer = agent.run(\"What meetings do I have tomorrow?\")\n",
      "print(answer)\n",
      "```\n",
      "\n",
      "*You don‚Äôt need to understand every line right away‚ÄîLangChain‚Äôs documentation walks you through each piece.*\n",
      "\n",
      "---\n",
      "\n",
      "## 5. When would you use LangChain?\n",
      "\n",
      "- **Chatbots** that need to look up facts, run calculations, or keep context.  \n",
      "- **Question‚Äëanswering over private data** (e.g., ‚ÄúWhat‚Äôs the status of project X?‚Äù using your company‚Äôs docs).  \n",
      "- **Automation assistants** that can send emails, schedule meetings, or control APIs.  \n",
      "- **Creative pipelines** (write a story, then generate images, then format a PDF).  \n",
      "\n",
      "If your app only needs ‚Äúgive me a text response‚Äù you can call the LLM directly. If you need *anything else*‚Äîmemory, external data, multi‚Äëstep reasoning‚ÄîLangChain makes it much easier.\n",
      "\n",
      "---\n",
      "\n",
      "## 6. Quick checklist for a beginner\n",
      "\n",
      "| ‚úÖ | Step | What to do |\n",
      "|---|------|------------|\n",
      "| 1 | **Pick a language model** | Sign up for OpenAI, Anthropic, etc., and get an API key. |\n",
      "| 2 | **Install LangChain** | `pip install langchain` (and `openai` or other provider libs). |\n",
      "| 3 | **Write a simple prompt** | ‚ÄúExplain photosynthesis in one sentence.‚Äù |\n",
      "| 4 | **Wrap it in a Chain** | Use `LLMChain` or `ChatPromptTemplate`. |\n",
      "| 5 | **Add a tool (optional)** | Define a Python function and register it as a `Tool`. |\n",
      "| 6 | **Create an Agent (optional)** | Let the LLM decide when to call the tool. |\n",
      "| 7 | **Run & iterate** | Test, tweak prompts, add memory if you need context. |\n",
      "\n",
      "---\n",
      "\n",
      "## 7. TL;DR (the ultra‚Äëshort version)\n",
      "\n",
      "- **LangChain = a set of LEGO‚Äëstyle pieces for building apps that use language models.**  \n",
      "- It lets you combine prompts, memory, external tools, and decision‚Äëmaking agents without writing all the plumbing yourself.  \n",
      "- You start with a simple ‚ÄúLLM + prompt‚Äù and then add more blocks (chains, tools, memory) as your app gets smarter.\n",
      "\n",
      "Give it a try with a tiny ‚Äúhello‚Äëworld‚Äù chatbot, then gradually add a tool (like a weather API) and see how the same framework handles the extra step automatically. That‚Äôs the power of LangChain in a nutshell!\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt\n",
    ")\n",
    "\n",
    "output = chain.run(\"LangChain\")\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042ad2ec",
   "metadata": {},
   "source": [
    "### Sequential Chains (MULTI-STEP THINKING)\n",
    "Real problems need steps.\n",
    "\n",
    "Example:\n",
    "- Explain topic\n",
    "- Summarize explanation\n",
    "\n",
    "```Output of one chain ‚Üí input of next```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1b6a381",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moiza\\AppData\\Local\\Temp\\ipykernel_2180\\63505215.py:38: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = overall_chain({\"topic\": \"LangChain\"})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'topic': 'LangChain', 'explanation': '## LangChain ‚Äì A Deep‚ÄëDive Overview  \\n\\nLangChain is an **open‚Äësource framework** that makes it easier to build **applications powered by large language models (LLMs)**.  \\nIt does this by providing a **modular, composable set of building blocks** that handle the repetitive, ‚Äúglue‚Äëcode‚Äù tasks that arise when you want an LLM to:\\n\\n1. **Understand and generate text** (prompt‚Äëengineering, LLM calls).  \\n2. **Interact with external data or tools** (search, databases, APIs, code execution).  \\n3. **Maintain state or memory** across turns.  \\n4. **Be orchestrated in flexible pipelines** (chains, agents, agents‚Äëwith‚Äëtools).  \\n\\nThink of LangChain as the **‚Äústandard library‚Äù for LLM‚Äëcentric software**, analogous to how Flask/Django are for web apps or Pandas for data manipulation.  \\n\\nBelow is a **structured, detailed guide** that covers:\\n\\n| Section | What you‚Äôll learn |\\n|---------|-------------------|\\n| 1Ô∏è‚É£  What problem does LangChain solve? | Why a framework is needed beyond raw LLM APIs |\\n| 2Ô∏è‚É£  Core concepts & terminology | LLMs, Prompts, PromptTemplates, Chains, Agents, Memory, Tools, Callbacks, etc. |\\n| 3Ô∏è‚É£  Architecture & data flow | How the pieces fit together |\\n| 4Ô∏è‚É£  The main modules (Python & TypeScript) | `langchain`, `langchain-community`, `langchain-core`, `langchain-experimental` |\\n| 5Ô∏è‚É£  Building blocks in practice | Code snippets for each block |\\n| 6Ô∏è‚É£  Typical use‚Äëcases & patterns | Retrieval‚Äëaugmented generation, chat assistants, autonomous agents, data‚Äëcentric pipelines |\\n| 7Ô∏è‚É£  Extending LangChain | Custom LLM wrappers, custom tools, custom memory, custom callbacks |\\n| 8Ô∏è‚É£  Deployment & production considerations | Caching, streaming, observability, scaling, security |\\n| 9Ô∏è‚É£  Ecosystem & community | Packages, integrations, tutorials, best‚Äëpractice resources |\\n| üîü  Quick‚Äëstart cheat‚Äësheet | Minimal runnable example |\\n\\n---\\n\\n## 1Ô∏è‚É£ Why LangChain Exists ‚Äì The ‚ÄúGlue‚ÄëCode‚Äù Problem\\n\\nWhen you start using an LLM (OpenAI, Anthropic, Cohere, Llama‚Äë2, etc.) you quickly discover that **the interesting logic lives outside the model**:\\n\\n| Task | Typical manual code (pseudo) |\\n|------|------------------------------|\\n| **Prompt engineering** | `prompt = f\"Answer the question: {question}\"` |\\n| **Calling the model** | `response = openai.ChatCompletion.create(messages=[{\"role\":\"user\",\"content\":prompt}])` |\\n| **Parsing the response** | `answer = json.loads(response[\"choices\"][0][\"message\"][\"content\"])` |\\n| **Fetching external data** | `search_results = google.search(query)` |\\n| **Combining data + LLM** | `prompt = f\"Based on these docs: {search_results} answer: {question}\"` |\\n| **Keeping conversation context** | `history.append({\"role\":\"assistant\",\"content\":answer})` |\\n| **Retry / error handling** | `while not success: try: ‚Ä¶ except RateLimitError: backoff()` |\\n| **Observability** | `log(prompt, response, latency)` |\\n\\nDoing all of this **repeatedly** across projects leads to:\\n\\n* **Duplicated code** across teams.  \\n* **Hard‚Äëto‚Äëmaintain pipelines** (any change in one piece ripples through many scripts).  \\n* **Inconsistent best practices** (e.g., some projects cache, others don‚Äôt).  \\n\\n**LangChain‚Äôs mission** is to **abstract, standardize, and compose** these patterns so you can focus on *application logic* rather than *infrastructure plumbing*.\\n\\n---\\n\\n## 2Ô∏è‚É£ Core Concepts & Terminology  \\n\\nBelow is the **vocabulary** you‚Äôll encounter when reading LangChain docs or code. Understanding each piece helps you decide how to assemble them.\\n\\n| Concept | Description | Typical Class (Python) |\\n|---------|-------------|------------------------|\\n| **LLM** | Wrapper around a language model provider (OpenAI, Azure, HuggingFace, etc.). Handles token limits, streaming, retries. | `OpenAI`, `ChatOpenAI`, `Anthropic`, `HuggingFaceHub` |\\n| **PromptTemplate** | A reusable string with placeholders (`{question}`, `{context}`) that can be rendered into a concrete prompt. | `PromptTemplate` |\\n| **ChatPromptTemplate** | Same as PromptTemplate but for chat‚Äëstyle messages (system, user, assistant). | `ChatPromptTemplate` |\\n| **LLMChain** | The simplest chain: **PromptTemplate ‚Üí LLM ‚Üí OutputParser**. It ties a prompt to an LLM and optionally parses the response. | `LLMChain` |\\n| **Chain** | A **pipeline of components** where the output of one step becomes the input of the next. Chains can be linear, branched, or conditional. | `SequentialChain`, `RouterChain`, custom `Chain` subclass |\\n| **Agent** | An **autonomous decision‚Äëmaker** that can **choose** which tool to call based on the user‚Äôs query and the LLM‚Äôs reasoning. Think ‚ÄúChatGPT with plugins‚Äù. | `AgentExecutor`, `ZeroShotAgent`, `ConversationalAgent` |\\n| **Tool** | A **functionality** that an agent can invoke (search, calculator, database query, code execution). Each tool has a name, description, and a callable. | `Tool`, `StructuredTool`, `BaseTool` |\\n| **Memory** | A **stateful component** that stores past interactions (chat history, key‚Äëvalue pairs, vector store embeddings) and injects them into prompts. | `ConversationBufferMemory`, `VectorStoreRetrieverMemory`, `SummaryMemory` |\\n| **Retriever** | A **search interface** that returns relevant documents given a query. Often backed by a vector store (FAISS, Pinecone, Weaviate). | `FAISS`, `Pinecone`, `ElasticVectorSearch` |\\n| **RetrieverChain** | Combines a **Retriever** with an **LLM** to perform **Retrieval‚ÄëAugmented Generation (RAG)**. | `RetrievalQAChain` |\\n| **Callback** | Hook points that fire on **events** (LLM start/end, chain start/end, tool call). Used for logging, tracing, streaming UI, token usage tracking. | `CallbackManager`, `StdOutCallbackHandler`, `OpenTelemetryCallbackHandler` |\\n| **Schema / OutputParser** | A **structured output definition** (e.g., JSON schema, Pydantic model) that the LLM must obey. The parser validates and converts raw text into Python objects. | `JsonOutputParser`, `PydanticOutputParser` |\\n| **Eval** | Utilities for **benchmarking** LLM responses against ground‚Äëtruth data (accuracy, relevance, hallucination detection). | `LLMEvalChain`, `EvaluationResult` |\\n| **LangChain Hub** | A **public repository** of community‚Äëshared prompts, chains, agents, and tools that can be imported with a single URL. | `load_prompt`, `load_chain` |\\n\\n> **Tip:** In the TypeScript/Node ecosystem the same concepts exist under `@langchain/*` packages (e.g., `@langchain/openai`, `@langchain/core`, `@langchain/agents`). The API surface is intentionally similar.\\n\\n---\\n\\n## 3Ô∏è‚É£ Architecture & Data Flow  \\n\\nBelow is a **high‚Äëlevel diagram** (textual) of a typical LangChain request:\\n\\n```\\nUser Input\\n   ‚îÇ\\n   ‚ñº\\n[Memory] ‚îÄ‚îÄ‚ñ∫ (inject past context) ‚îÄ‚îÄ‚ñ∫\\n   ‚îÇ\\n   ‚ñº\\n[PromptTemplate]  (render with variables) ‚îÄ‚îÄ‚ñ∫\\n   ‚îÇ\\n   ‚ñº\\n[LLM]  (call provider) ‚îÄ‚îÄ‚ñ∫ raw LLM response\\n   ‚îÇ\\n   ‚ñº\\n[OutputParser] (optional) ‚îÄ‚îÄ‚ñ∫ structured data\\n   ‚îÇ\\n   ‚ñº\\n[CallbackManager] (emit events) ‚îÄ‚îÄ‚ñ∫ logging / UI streaming\\n   ‚îÇ\\n   ‚ñº\\n[Agent / Chain] (may loop back, call Tools, retrieve docs, etc.)\\n   ‚îÇ\\n   ‚ñº\\nFinal Answer ‚Üí User\\n```\\n\\n- **Memory** can be *pre‚Äë* or *post‚Äë*prompt (e.g., a conversation buffer added to the system message).  \\n- **PromptTemplate** can be a **ChatPromptTemplate** that yields a list of messages (`system`, `user`, `assistant`).  \\n- **LLM** may be a **Chat model** (`gpt‚Äë4o`) or a **completion model** (`text‚Äëdavinci‚Äë003`).  \\n- **OutputParser** can enforce JSON, XML, or a Pydantic model, reducing hallucination.  \\n- **CallbackManager** is the central hub for observability; you can attach multiple handlers (stdout, file, OpenTelemetry, LangSmith).  \\n- **Agent** decides whether to **just answer** or **invoke a Tool** (e.g., ‚Äúsearch the web‚Äù). If a tool is called, the result is fed back into the chain, possibly iterating several times.  \\n\\n---\\n\\n## 4Ô∏è‚É£ The Main Modules (Python)  \\n\\nLangChain has been split into **core** and **community** packages to keep the core lightweight while allowing rapid integration of new providers.\\n\\n| Package | Purpose | Typical Imports |\\n|---------|---------|-----------------|\\n| `langchain-core` | **Fundamental abstractions** (`LLM`, `PromptTemplate`, `Chain`, `CallbackManager`). No external provider dependencies. | `from langchain_core.prompts import PromptTemplate` |\\n| `langchain-community` | **Integrations** with third‚Äëparty services (vector stores, APIs, document loaders). | `from langchain_community.vectorstores import FAISS` |\\n| `langchain` (meta‚Äëpackage) | Re‚Äëexports the most common classes for convenience (`OpenAI`, `LLMChain`, `ConversationChain`). | `from langchain import OpenAI, LLMChain` |\\n| `langchain-experimental` | **Cutting‚Äëedge features** (e.g., LangSmith tracing, advanced agents). | `from langchain_experimental.agents import create_openai_functions_agent` |\\n| `langchain-hub` | Utilities to **load** prompts/chains from the LangChain Hub. | `from langchain_hub import load_prompt` |\\n\\n**TypeScript/Node** mirrors this structure with `@langchain/core`, `@langchain/openai`, `@langchain/community`, etc.\\n\\n---\\n\\n## 5Ô∏è‚É£ Building Blocks in Practice  \\n\\nBelow are **minimal, runnable snippets** (Python 3.10+) that illustrate each core component.  \\nYou can copy‚Äëpaste them into a fresh virtual environment (`pip install \"langchain[all]\" openai`) and run.\\n\\n### 5.1 PromptTemplate & LLMChain\\n\\n```python\\nfrom langchain import OpenAI, LLMChain\\nfrom langchain.prompts import PromptTemplate\\n\\n# 1Ô∏è‚É£ Define a reusable prompt\\ntemplate = \"\"\"You are a helpful assistant.\\nAnswer the following question in one sentence.\\n\\nQuestion: {question}\\nAnswer:\"\"\"\\nprompt = PromptTemplate.from_template(template)\\n\\n# 2Ô∏è‚É£ Wrap an LLM (OpenAI\\'s gpt‚Äë3.5‚Äëturbo)\\nllm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0)\\n\\n# 3Ô∏è‚É£ Create the chain\\nchain = LLMChain(llm=llm, prompt=prompt)\\n\\n# 4Ô∏è‚É£ Run it\\nprint(chain.invoke({\"question\": \"What is the capital of France?\"}))\\n```\\n\\n**Output (example)**  \\n```\\n{\\'text\\': \\'The capital of France is Paris.\\'}\\n```\\n\\n### 5.2 ChatPromptTemplate + Conversation Memory\\n\\n```python\\nfrom langchain.memory import ConversationBufferMemory\\nfrom langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.chains import ConversationChain\\n\\n# Memory that keeps the last 5 turns\\nmemory = ConversationBufferMemory(k=5)\\n\\n# Chat prompt (system + user)\\nsystem_msg = SystemMessagePromptTemplate.from_template(\\n    \"You are a friendly tutor. Keep answers concise.\"\\n)\\nhuman_msg = HumanMessagePromptTemplate.from_template(\"{input}\")\\n\\nchat_prompt = ChatPromptTemplate.from_messages([system_msg, human_msg])\\n\\n# Chat model\\nchat = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2)\\n\\n# Conversation chain that automatically injects memory\\nconv_chain = ConversationChain(\\n    llm=chat,\\n    prompt=chat_prompt,\\n    memory=memory,\\n    verbose=True,\\n)\\n\\n# Simulate a short dialogue\\nprint(conv_chain.invoke({\"input\": \"What is photosynthesis?\"}))\\nprint(conv_chain.invoke({\"input\": \"Why is it important for Earth?\"}))\\n```\\n\\n### 5.3 Retrieval‚ÄëAugmented Generation (RAG)\\n\\n```python\\nfrom langchain_community.document_loaders import TextLoader\\nfrom langchain_community.vectorstores import FAISS\\nfrom langchain.embeddings import OpenAIEmbeddings\\nfrom langchain.chains import RetrievalQA\\nfrom langchain.prompts import PromptTemplate\\n\\n# 1Ô∏è‚É£ Load a small knowledge base\\nloader = TextLoader(\"data/faq.txt\")   # each line = a FAQ entry\\ndocs = loader.load_and_split()\\n\\n# 2Ô∏è‚É£ Embed and index with FAISS (in‚Äëmemory)\\nembeddings = OpenAIEmbeddings()\\nvectorstore = FAISS.from_documents(docs, embeddings)\\n\\n# 3Ô∏è‚É£ Retriever (top‚Äëk = 4)\\nretriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\\n\\n# 4Ô∏è‚É£ Prompt that tells the model to use retrieved docs\\nrag_prompt = PromptTemplate.from_template(\\n    \"\"\"You are an expert assistant. Use the following retrieved context to answer the question.\\n    \\nContext:\\n{context}\\n    \\nQuestion: {question}\\nAnswer (concise, cite sources if possible):\"\"\"\\n)\\n\\n# 5Ô∏è‚É£ Build the RetrievalQA chain\\nqa = RetrievalQA.from_chain_type(\\n    llm=OpenAI(model=\"gpt-3.5-turbo\"),\\n    chain_type=\"stuff\",               # \"stuff\" = concatenate docs\\n    retriever=retriever,\\n    return_source_documents=True,\\n    chain_type_kwargs={\"prompt\": rag_prompt},\\n)\\n\\n# 6Ô∏è‚É£ Ask a question\\nresult = qa({\"query\": \"How do I reset my router?\"})\\nprint(result[\"answer\"])\\nprint(\"\\\\nSources:\")\\nfor doc in result[\"source_documents\"]:\\n    print(\"- \", doc.metadata.get(\"source\"))\\n```\\n\\n### 5.4 Agent with Tools (Zero‚ÄëShot React)\\n\\n```python\\nfrom langchain.agents import initialize_agent, Tool\\nfrom langchain.tools import DuckDuckGoSearchRun\\nfrom langchain.chat_models import ChatOpenAI\\n\\n# Define a simple search tool\\nsearch_tool = Tool(\\n    name=\"Search\",\\n    func=DuckDuckGoSearchRun().run,\\n    description=\"Useful for answering questions about current events or obscure facts.\"\\n)\\n\\n# Initialize a zero‚Äëshot agent that can decide to call the tool\\nagent = initialize_agent(\\n    tools=[search_tool],\\n    llm=ChatOpenAI(model=\"gpt-4o-mini\", temperature=0),\\n    agent=\"zero-shot-react-description\",\\n    verbose=True,\\n)\\n\\n# Ask a multi‚Äëstep question\\nresponse = agent.run(\\n    \"What is the latest version of LangChain, and give me a short summary of its new features.\"\\n)\\nprint(response)\\n```\\n\\nThe agent will **first call the search tool**, retrieve the latest release notes, then **compose a concise summary**.\\n\\n### 5.5 Custom OutputParser (JSON + Pydantic)\\n\\n```python\\nfrom pydantic import BaseModel, Field\\nfrom langchain.output_parsers import PydanticOutputParser\\nfrom langchain.prompts import PromptTemplate\\nfrom langchain import OpenAI\\n\\nclass WeatherReport(BaseModel):\\n    location: str = Field(..., description=\"City name\")\\n    temperature_c: float = Field(..., description=\"Current temperature in Celsius\")\\n    condition: str = Field(..., description=\"One‚Äëword weather condition (e.g., sunny, rainy)\")\\n\\nparser = PydanticOutputParser(pydantic_object=WeatherReport)\\n\\nprompt = PromptTemplate(\\n    template=\"\"\"You are a weather bot. Return a JSON object that matches this schema:\\n{format_instructions}\\n\\nUser query: {question}\\n\"\"\",\\n    input_variables=[\"question\"],\\n    partial_variables={\"format_instructions\": parser.get_format_instructions()},\\n)\\n\\nchain = LLMChain(\\n    llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0),\\n    prompt=prompt,\\n    output_parser=parser,\\n)\\n\\nresult = chain.invoke({\"question\": \"What\\'s the weather like in Tokyo right now?\"})\\nprint(result)   # -> WeatherReport instance\\n```\\n\\nThe parser **guarantees** that the LLM output conforms to the schema, raising a validation error otherwise.\\n\\n### 5.6 Callbacks for Streaming UI\\n\\n```python\\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\\nfrom langchain.chat_models import ChatOpenAI\\n\\nstream_handler = StreamingStdOutCallbackHandler()\\nllm = ChatOpenAI(\\n    model=\"gpt-4o-mini\",\\n    temperature=0,\\n    streaming=True,\\n    callbacks=[stream_handler],\\n)\\n\\n# Simple streaming call\\nresponse = llm.invoke([(\"human\", \"Explain quantum entanglement in 2 sentences.\")])\\nprint(\"\\\\n--- Done ---\")\\n```\\n\\nThe `StreamingStdOutCallbackHandler` prints each token as it arrives, enabling **real‚Äëtime UI** experiences.\\n\\n---\\n\\n## 6Ô∏è‚É£ Typical Use‚ÄëCases & Design Patterns  \\n\\n| Use‚ÄëCase | Recommended LangChain Pattern | Why it fits |\\n|----------|------------------------------|-------------|\\n| **Chatbot with long‚Äëterm memory** | `ConversationChain` + `ConversationBufferMemory` or `VectorStoreRetrieverMemory` | Memory injects prior turns, preventing the model from ‚Äúforgetting‚Äù. |\\n| **FAQ bot over a static knowledge base** | `RetrievalQAChain` (RAG) + `FAISS`/`Pinecone` | Retrieves relevant docs, then LLM generates answer. |\\n| **Dynamic tool‚Äëdriven assistant (e.g., ‚Äúschedule meeting‚Äù, ‚Äúrun code‚Äù)** | `AgentExecutor` + custom `Tool`s (e.g., `PythonREPLTool`, `APICallTool`) | Agent decides when to call external functions. |\\n| **Data extraction / structured output** | `PromptTemplate` + `JsonOutputParser` or `PydanticOutputParser` | Guarantees parsable JSON, reduces hallucination. |\\n| **Multi‚Äëmodal pipelines (text ‚Üí image ‚Üí text)** | Chain that calls a **text‚Äëto‚Äëimage** model, stores image URL, then passes it to a **captioning** LLM. | Chains can be arbitrarily composed; each step can be a different model. |\\n| **Evaluation & benchmarking** | `LLMEvalChain` + custom dataset loader | Automates scoring of LLM responses against ground truth. |\\n| **Observability & tracing** | `CallbackManager` + `LangSmith` (or OpenTelemetry) | Centralized logging of token usage, latency, and tool calls. |\\n| **Serverless API endpoint** | `FastAPI` + LangChain chain wrapped in a single function | Minimal boilerplate; chain is stateless (or uses external memory store). |\\n\\n### Pattern: ‚Äú**Retriever ‚Üí LLM ‚Üí Post‚ÄëProcessor**‚Äù\\n\\n```\\nUser Query\\n   ‚îÇ\\n   ‚ñº\\nRetriever (vector store) ‚Üí list[Document]\\n   ‚îÇ\\n   ‚ñº\\nPromptTemplate (inject docs) ‚Üí prompt\\n   ‚îÇ\\n   ‚ñº\\nLLM (Chat) ‚Üí raw answer\\n   ‚îÇ\\n   ‚ñº\\nPost‚ÄëProcessor (e.g., regex, JSON parser) ‚Üí final output\\n```\\n\\n### Pattern: ‚Äú**Agent ‚Üí Tool ‚Üí Feedback Loop**‚Äù\\n\\n```\\nUser Query\\n   ‚îÇ\\n   ‚ñº\\nAgent (LLM + reasoning) ‚Üí decides \"call Tool X\"\\n   ‚îÇ\\n   ‚ñº\\nTool X (function) ‚Üí result\\n   ‚îÇ\\n   ‚ñº\\nAgent receives result ‚Üí may call another tool or produce final answer\\n```\\n\\n---\\n\\n## 7Ô∏è‚É£ Extending LangChain  \\n\\n### 7.1 Custom LLM Wrapper  \\n\\nIf you have an in‚Äëhouse model served via a REST endpoint:\\n\\n```python\\nfrom langchain_core.language_models import BaseLLM\\nfrom langchain_core.outputs import Generation, LLMResult\\nimport httpx\\n\\nclass MyRestLLM(BaseLLM):\\n    \"\"\"Wraps a custom HTTP‚Äëbased LLM.\"\"\"\\n    def __init__(self, endpoint: str, api_key: str, temperature: float = 0.0):\\n        self.endpoint = endpoint\\n        self.api_key = api_key\\n        self.temperature = temperature\\n\\n    @property\\n    def _identifying_params(self):\\n        return {\"endpoint\": self.endpoint, \"temperature\": self.temperature}\\n\\n    @property\\n    def _llm_type(self):\\n        return \"my-rest-llm\"\\n\\n    def _call(self, prompt: str, stop: list[str] | None = None) -> str:\\n        payload = {\"prompt\": prompt, \"temperature\": self.temperature}\\n        headers = {\"Authorization\": f\"Bearer {self.api_key}\"}\\n        resp = httpx.post(self.endpoint, json=payload, headers=headers, timeout=30)\\n        resp.raise_for_status()\\n        return resp.json()[\"generated_text\"]\\n```\\n\\nNow you can use `MyRestLLM` anywhere a regular LLM is expected.\\n\\n### 7.2 Custom Tool  \\n\\n```python\\nfrom langchain.tools import BaseTool\\nimport subprocess, json\\n\\nclass BashTool(BaseTool):\\n    name = \"bash\"\\n    description = \"Executes a bash command and returns its stdout. Use for quick OS tasks.\"\\n\\n    def _run(self, command: str) -> str:\\n        result = subprocess.run(\\n            command, shell=True, capture_output=True, text=True, timeout=10\\n        )\\n        if result.returncode != 0:\\n            raise RuntimeError(f\"Error: {result.stderr}\")\\n        return result.stdout.strip()\\n```\\n\\nAdd it to an agent:\\n\\n```python\\nagent = initialize_agent(\\n    tools=[BashTool()],\\n    llm=ChatOpenAI(model=\"gpt-4o-mini\"),\\n    agent=\"zero-shot-react-description\",\\n)\\n```\\n\\n### 7.3 Custom Memory (e.g., Redis‚Äëbacked)  \\n\\n```python\\nfrom langchain.memory import BaseMemory\\nimport redis\\nimport json\\n\\nclass RedisConversationMemory(BaseMemory):\\n    \"\"\"Stores conversation history in a Redis list.\"\"\"\\n    def __init__(self, redis_url: str, session_id: str):\\n        self.client = redis.from_url(redis_url)\\n        self.session_id = session_id\\n\\n    @property\\n    def memory_variables(self):\\n        return [\"history\"]\\n\\n    def load_memory_variables(self, inputs: dict) -> dict:\\n        raw = self.client.lrange(self.session_id, 0, -1)\\n        history = [json.loads(item) for item in raw]\\n        return {\"history\": history}\\n\\n    def save_context(self, inputs: dict, outputs: dict) -> None:\\n        entry = {\"input\": inputs, \"output\": outputs}\\n        self.client.rpush(self.session_id, json.dumps(entry))\\n        # optional: trim to last N entries\\n        self.client.ltrim(self.session_id, -10, -1)\\n```\\n\\nPlug it into a chain:\\n\\n```python\\nmemory = RedisConversationMemory(redis_url=\"redis://localhost:6379\", session_id=\"user:123\")\\nchain = LLMChain(llm=OpenAI(), prompt=prompt, memory=memory)\\n```\\n\\n### 7.4 Custom Callback (e.g., send metrics to Prometheus)\\n\\n```python\\nfrom langchain.callbacks.base import BaseCallbackHandler\\nimport time, prometheus_client\\n\\nclass PrometheusCallback(BaseCallbackHandler):\\n    def __init__(self):\\n        self.latency_hist = prometheus_client.Histogram(\\n            \"langchain_llm_latency_seconds\", \"LLM call latency\"\\n        )\\n        self.token_counter = prometheus_client.Counter(\\n            \"langchain_llm_tokens_total\", \"Total tokens used\", [\"model\"]\\n        )\\n        self._start = None\\n\\n    def on_llm_start(self, serialized, prompts, **kwargs):\\n        self._start = time.time()\\n\\n    def on_llm_end(self, response, **kwargs):\\n        elapsed = time.time() - self._start\\n        self.latency_hist.observe(elapsed)\\n        model = response.llm_output.get(\"model_name\", \"unknown\")\\n        self.token_counter.labels(model=model).inc(response.llm_output.get(\"token_usage\", 0))\\n```\\n\\nAdd to any LLM:\\n\\n```python\\nllm = OpenAI(callbacks=[PrometheusCallback()], temperature=0)\\n```\\n\\n---\\n\\n## 8Ô∏è‚É£ Deployment & Production Considerations  \\n\\n| Concern | LangChain‚Äëspecific Guidance |\\n|---------|-----------------------------|\\n| **Stateless vs Stateful** | Chains themselves are stateless; **Memory** and **Vector Stores** hold state. Deploy memory in a durable store (Redis, PostgreSQL, Pinecone). |\\n| **Caching** | Use `langchain.cache` (e.g., `InMemoryCache`, `RedisCache`) to avoid repeated LLM calls for identical prompts. |\\n| **Streaming** | Set `streaming=True` on the LLM and attach a `StreamingStdOutCallbackHandler` or a custom WebSocket handler. |\\n| **Rate‚Äëlimit handling** | LangChain‚Äôs `LLM` wrappers already implement exponential back‚Äëoff; you can customize via `max_retries` and `retry_interval`. |\\n| **Observability** | Enable **LangSmith** (LangChain‚Äôs SaaS tracing platform) or plug in OpenTelemetry callbacks. |\\n| **Security** | - **Sanitize tool inputs** (especially when exposing a Bash or SQL tool). <br> - **Redact PII** in callbacks before logging. <br> - Use **private VPC endpoints** for vector stores. |\\n| **Scalability** | - Horizontal scale the API layer (FastAPI, Flask, or serverless). <br> - Keep vector store separate (managed service). <br> - For heavy LLM traffic, consider **batching** or **model parallelism** (e.g., using vLLM). |\\n| **Testing** | LangChain ships with `langchain.testing` utilities: `FakeLLM`, `FakeChatModel`, `FakeRetriever`. Use them for unit tests. |\\n| **Versioning** | Pin both **LangChain** and **LLM provider** versions. Breaking changes are announced in the changelog; use `pip install \"langchain==0.2.*\"` for stability. |\\n\\n### Example: FastAPI endpoint with LangChain + Redis Memory\\n\\n```python\\nfrom fastapi import FastAPI, Request\\nfrom pydantic import BaseModel\\nfrom langchain import OpenAI, LLMChain\\nfrom langchain.prompts import PromptTemplate\\nfrom langchain.memory import ConversationBufferMemory\\nimport redis\\n\\napp = FastAPI()\\nredis_client = redis.from_url(\"redis://localhost:6379\")\\n\\nclass ChatRequest(BaseModel):\\n    session_id: str\\n    message: str\\n\\ndef get_memory(session_id: str):\\n    # Simple wrapper that uses a Redis list as a buffer\\n    class RedisMemory(ConversationBufferMemory):\\n        def __init__(self):\\n            super().__init__(k=10)\\n            self.session_id = session_id\\n\\n        def load_memory_variables(self, inputs):\\n            raw = redis_client.lrange(self.session_id, 0, -1)\\n            history = [json.loads(item) for item in raw]\\n            return {\"history\": \"\\\\n\".join([f\"Human: {h[\\'input\\']}\\\\nAI: {h[\\'output\\']}\" for h in history])}\\n\\n        def save_context(self, inputs, outputs):\\n            entry = json.dumps({\"input\": inputs[\"input\"], \"output\": outputs[\"output\"]})\\n            redis_client.rpush(self.session_id, entry)\\n            redis_client.ltrim(self.session_id, -10, -1)\\n\\n    return RedisMemory()\\n\\nprompt = PromptTemplate.from_template(\\n    \"\"\"You are a helpful assistant. Keep the conversation friendly.\\n\\n{history}\\nHuman: {input}\\nAI:\"\"\"\\n)\\n\\nllm = OpenAI(model=\"gpt-4o-mini\", temperature=0.2)\\n\\n@app.post(\"/chat\")\\nasync def chat_endpoint(req: ChatRequest):\\n    memory = get_memory(req.session_id)\\n    chain = LLMChain(llm=llm, prompt=prompt, memory=memory)\\n    result = chain.invoke({\"input\": req.message})\\n    return {\"answer\": result[\"text\"]}\\n```\\n\\nDeploy with `uvicorn main:app --host 0.0.0.0 --port 8000`. The endpoint is **stateless** from the API perspective; all conversation state lives in Redis.\\n\\n---\\n\\n## 9Ô∏è‚É£ Ecosystem & Community  \\n\\n| Resource | What you‚Äôll find |\\n|----------|------------------|\\n| **LangChain Hub** (`https://smith.langchain.com/hub`) | Hundreds of community‚Äëshared prompts, chains, agents, and tool definitions. Import with `load_prompt(\"hub://...\")`. |\\n| **LangSmith** | SaaS platform for tracing, evaluating, and version‚Äëcontrolling chains. Free tier for small projects. |\\n| **Discord & Slack** | Active developer community (`#langchain` Discord). Quick help, showcase projects, and contribution guidelines. |\\n| **GitHub** | `langchain-ai/langchain` (Python) and `langchain-ai/langchainjs` (TypeScript). Issues are triaged weekly; good place for feature requests. |\\n| **Tutorials & Notebooks** | Official docs contain step‚Äëby‚Äëstep notebooks for RAG, agents, evaluation, and multi‚Äëmodal pipelines. |\\n| **Integrations** | Over 30 vector stores, 20 document loaders, 15 tool wrappers (Google Search, Wolfram Alpha, Zapier, etc.). |\\n| **Contributing** | Follow the `CONTRIBUTING.md` ‚Äì you can add a new tool, a new vector store wrapper, or improve docs. PRs are welcomed. |\\n\\n---\\n\\n## üîü Quick‚ÄëStart Cheat‚ÄëSheet  \\n\\n| Goal | One‚Äëliner (Python) |\\n|------|--------------------|\\n| **Basic LLM call** | `OpenAI(model=\"gpt-3.5-turbo\").invoke(\"Explain recursion in one sentence.\")` |\\n| **PromptTemplate + LLMChain** | `LLMChain(OpenAI(), PromptTemplate.from_template(\"Q: {q}\\\\nA:\")).invoke({\"q\":\"What is AI?\"})` |\\n| **Chat with memory** | `ConversationChain(llm=ChatOpenAI(), memory=ConversationBufferMemory()).invoke({\"input\":\"Hi!\"})` |\\n| **RAG (FAISS)** | `RetrievalQA.from_chain_type(OpenAI(), retriever=FAISS.from_documents(docs, OpenAIEmbeddings()).as_retriever()).run(\"...\")` |\\n| **Agent with a search tool** | `initialize_agent([DuckDuckGoSearchRun()], ChatOpenAI(), agent=\"zero-shot-react-description\").run(\"...\")` |\\n| **Streaming output** | `ChatOpenAI(streaming=True, callbacks=[StreamingStdOutCallbackHandler()]).invoke([...])` |\\n| **Load a Hub prompt** | `from langchain_hub import load_prompt; prompt = load_prompt(\"hub://my-org/my-prompt\")` |\\n| **Add a custom callback** | `llm = OpenAI(callbacks=[MyCallback()])` |\\n| **Cache LLM calls** | `from langchain.cache import InMemoryCache; OpenAI(cache=InMemoryCache()).invoke(\"...\")` |\\n| **Run a chain in FastAPI** | See the FastAPI example above ‚Äì just wrap a `LLMChain` in an endpoint. |\\n\\n---\\n\\n## üìö TL;DR Summary  \\n\\n* **LangChain** is a **modular framework** that abstracts the repetitive plumbing needed to build LLM‚Äëcentric applications.  \\n* Its **core abstractions**‚ÄîPromptTemplate, LLM, Chain, Agent, Tool, Memory, Retriever, Callback‚Äîcan be **mixed & matched** to create anything from a simple chatbot to a fully autonomous agent that calls APIs, runs code, and stores state.  \\n* The **Python ecosystem** is split into `langchain-core` (bare‚Äëbones abstractions), `langchain-community` (integrations), and `langchain` (convenient meta‚Äëpackage). The **TypeScript side** mirrors this design.  \\n* **RAG**, **agents**, **structured output**, **streaming**, and **observability** are first‚Äëclass features, each with ready‚Äëmade implementations.  \\n* Extending LangChain is straightforward: write a custom LLM wrapper, tool, memory, or callback, and plug it into any chain or agent.  \\n* Production‚Äëready deployments rely on **external state stores** (Redis, vector DBs), **caching**, **streaming**, and **observability** (LangSmith, OpenTelemetry).  \\n* The **LangChain Hub** and **LangSmith** provide reusable assets and tracing dashboards, while a vibrant community keeps the ecosystem fresh.\\n\\nWith this knowledge, you can now **design, prototype, and ship** sophisticated LLM‚Äëdriven products without reinventing the wheel for every new feature. Happy chaining! üöÄ', 'summary': '**LangChain ‚Äì Quick Overview**\\n\\n- **What it is:** An open‚Äësource, modular framework that supplies ‚Äústandard‚Äëlibrary‚Äù building blocks for applications that use large language models (LLMs). It handles the repetitive glue‚Äëcode so developers can focus on product logic.\\n\\n- **Why it‚Äôs needed:** Directly using LLM APIs requires a lot of boiler‚Äëplate (prompt engineering, API calls, response parsing, external data fetching, state‚Äëmanagement, error handling, logging). LangChain abstracts and composes these patterns, eliminating duplicated code and inconsistencies.\\n\\n- **Core concepts**\\n  - **LLM / ChatModel:** Wrapper around a model provider (OpenAI, Anthropic, HuggingFace, etc.).\\n  - **PromptTemplate / ChatPromptTemplate:** Reusable prompts with placeholders.\\n  - **LLMChain:** Prompt ‚Üí LLM ‚Üí optional OutputParser.\\n  - **Chain:** General pipeline of components (linear, branched, conditional).\\n  - **Agent:** Autonomous decision‚Äëmaker that can invoke **Tools** (search, calculator, DB, code execution) based on LLM reasoning.\\n  - **Tool:** Callable functionality exposed to agents.\\n  - **Memory:** Stores conversation or key‚Äëvalue state and injects it into prompts.\\n  - **Retriever / RetrievalQAChain:** Vector‚Äëstore search + LLM for Retrieval‚ÄëAugmented Generation (RAG).\\n  - **Callback / CallbackManager:** Hook points for logging, streaming, tracing (e.g., LangSmith, OpenTelemetry).\\n  - **OutputParser / Schema:** Enforces structured outputs (JSON, Pydantic models).\\n  - **Eval utilities:** Benchmark LLM responses.\\n\\n- **Architecture flow**\\n  1. **Memory** adds past context.  \\n  2. **PromptTemplate** renders a concrete prompt.  \\n  3. **LLM** generates a raw response.  \\n  4. **OutputParser** (optional) converts to structured data.  \\n  5. **Callbacks** emit events for observability.  \\n  6. **Agent/Chain** may loop, call tools, or return the final answer.\\n\\n- **Package layout (Python)**\\n  - `langchain-core`: core abstractions, no external deps.  \\n  - `langchain-community`: integrations (vector stores, document loaders, APIs).  \\n  - `langchain`: meta‚Äëpackage re‚Äëexporting common classes.  \\n  - `langchain-experimental`: cutting‚Äëedge features (LangSmith tracing, advanced agents).  \\n  - `langchain-hub`: utilities to load community‚Äëshared prompts/chains.\\n\\n- **Typical patterns & use‚Äëcases**\\n  - **Chatbot with memory** ‚Üí `ConversationChain` + `ConversationBufferMemory`.  \\n  - **FAQ / knowledge‚Äëbase bot** ‚Üí `RetrievalQAChain` + FAISS/Pinecone vector store.  \\n  - **Tool‚Äëdriven assistant** ‚Üí `AgentExecutor` + custom `Tool`s (search, code exec).  \\n  - **Structured data extraction** ‚Üí Prompt + `JsonOutputParser` / `PydanticOutputParser`.  \\n  - **Streaming UI** ‚Üí LLM with `streaming=True` + `StreamingStdOutCallbackHandler`.  \\n  - **Evaluation** ‚Üí `LLMEvalChain`.  \\n\\n- **Extending LangChain**\\n  - Write a custom LLM wrapper, tool, memory store (e.g., Redis‚Äëbacked), or callback (e.g., Prometheus metrics) and plug it into any chain/agent.\\n\\n- **Production considerations**\\n  - Keep state in external stores (Redis, vector DBs).  \\n  - Use caching (`InMemoryCache`, `RedisCache`).  \\n  - Enable streaming and back‚Äëoff for rate limits.  \\n  - Add observability via callbacks or LangSmith/OpenTelemetry.  \\n  - Secure tool inputs, redact PII, and run LLM calls behind private endpoints.  \\n  - Scale API layer (FastAPI, serverless) and vector store separately.\\n\\n- **Ecosystem**\\n  - **LangChain Hub:** community‚Äëshared prompts, chains, agents.  \\n  - **LangSmith:** tracing, evaluation, versioning SaaS platform.  \\n  - Active Discord/Slack community, extensive docs, notebooks, and over 30 vector‚Äëstore and 20 tool integrations.\\n\\n- **Quick‚Äëstart cheat‚Äësheet** (Python one‚Äëliners)\\n  - Basic LLM: `OpenAI(model=\"gpt-3.5-turbo\").invoke(\"Explain recursion.\")`\\n  - Prompt + chain: `LLMChain(OpenAI(), PromptTemplate.from_template(\"Q: {q}\\\\nA:\")).invoke({\"q\":\"What is AI?\"})`\\n  - Chat with memory: `ConversationChain(llm=ChatOpenAI(), memory=ConversationBufferMemory()).invoke({\"input\":\"Hi\"})`\\n  - RAG: `RetrievalQA.from_chain_type(OpenAI(), retriever=FAISS.from_documents(docs, OpenAIEmbeddings()).as_retriever()).run(\"...\")`\\n  - Agent with tool: `initialize_agent([DuckDuckGoSearchRun()], ChatOpenAI(), agent=\"zero-shot-react-description\").run(\"...\")`\\n  - Streaming: `ChatOpenAI(streaming=True, callbacks=[StreamingStdOutCallbackHandler()]).invoke([...])`\\n\\n**Bottom line:** LangChain supplies a composable, well‚Äëdocumented set of abstractions that turn raw LLM APIs into production‚Äëready pipelines‚Äîhandling prompting, memory, tool use, retrieval, observability, and extensibility‚Äîall while letting developers concentrate on the unique logic of their AI product.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import SequentialChain, LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Prompt 1\n",
    "explain_prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Explain {topic} in detail\"\n",
    ")\n",
    "\n",
    "# Prompt 2 (FIXED)\n",
    "summary_prompt = PromptTemplate(\n",
    "    input_variables=[\"explanation\"],\n",
    "    template=\"Summarize this text:\\n{explanation}\"\n",
    ")\n",
    "\n",
    "# Chain 1\n",
    "explain_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=explain_prompt,\n",
    "    output_key=\"explanation\"\n",
    ")\n",
    "\n",
    "# Chain 2\n",
    "summary_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=summary_prompt,\n",
    "    output_key=\"summary\"\n",
    ")\n",
    "\n",
    "# Sequential Chain\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[explain_chain, summary_chain],\n",
    "    input_variables=[\"topic\"],\n",
    "    output_variables=[\"explanation\", \"summary\"],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "result = overall_chain({\"topic\": \"LangChain\"})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726d32b8",
   "metadata": {},
   "source": [
    "### Memory\n",
    "LLMs forget everything.\n",
    "\n",
    "- Memory = conversation state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5305d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "volmatica",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
